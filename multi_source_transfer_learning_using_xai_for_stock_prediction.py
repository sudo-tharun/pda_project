# -*- coding: utf-8 -*-
"""multi-source-transfer-learning-using-XAI-for-stock-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18QPcH_nHEvurfAps0-A46-ZpDH8eEJT8

#### Imports
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install scikit-learn==1.0
!pip install lime
!pip install keras-tuner
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostRegressor
from keras.wrappers.scikit_learn import KerasRegressor
import keras_tuner as kt
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV
from math import sqrt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Bidirectional
import matplotlib.pyplot as plt
# %matplotlib inline

"""##### Setting seeds for random number generators"""

# tf.random.set_seed(42)
# np.random.seed(42)

source_data = pd.read_csv('./final-data/indicators_source_infy.csv', index_col=[0])
target_data = pd.read_csv('./final-data/indicators_target.csv', index_col=[0])
# source_data = pd.read_csv('./final-data/features_source_infy.csv')
# target_data = pd.read_csv('./final-data/features_target.csv')

source_data

target_data

"""#### Separating dates from source stock data"""

source_dates = source_data['Date']
source_data.drop(labels=['Date'], axis=1, inplace=True)

"""#### Separating date from target stock data"""

target_dates = target_data['Date']
target_data.drop(labels=['Date'], axis=1, inplace=True)
target_data

# temp
# dropping volume
source_data.drop(['Volume'], axis=1, inplace=True)
target_data.drop(['Volume'], axis=1, inplace=True)

"""#### Display columns of source stock data and target stock data"""

source_data_columns = source_data.columns
target_data_columns = target_data.columns
print(source_data_columns, target_data_columns, sep='\n')

"""#### Normalizing features"""

source_scaler = MinMaxScaler(feature_range=(0, 1))
source_scaler = source_scaler.fit(source_data.values)
source_data = source_scaler.transform(source_data.values)
source_data = pd.DataFrame(source_data, columns=source_data_columns)
source_data

target_scaler = MinMaxScaler(feature_range=(0, 1))
target_scaler = target_scaler.fit(target_data.values)
target_data = target_scaler.transform(target_data.values)
target_data = pd.DataFrame(target_data, columns=target_data_columns)
target_data

"""#### Separating target variable from features for source stock data"""

source_labels = source_data['Adj Close']
source_data.drop(labels=['Adj Close'], axis=1, inplace=True)
source_features = source_data
source_features

"""#### Separating target variables from features for target stock data"""

target_labels = target_data['Adj Close']
target_data.drop(labels=['Adj Close'], axis=1, inplace=True)
target_features = target_data
target_features

"""Each GRU layer requires input in the form of 3D numpy array.  
The array dimensions are used as follows:  
`[Number of samples, number of time steps, number of features]`  
To have array an length which can be used to create such dimensions,    
we will be limiting the number of records to a multiple of 100.
"""

print(f"Count of source features = {len(source_features)}, Count of target features = {len(target_features)}")

source_features = source_features.iloc[0:len(source_features)-len(source_features)%100]
source_labels = source_labels.iloc[0:len(source_labels)-len(source_labels)%100]
target_features = target_features.iloc[0:len(target_features)-len(target_features)%100]
target_labels = target_labels.iloc[0:len(target_labels)-len(target_labels)%100]
print(f"Count of source features = {len(source_features)} Count of target features = {len(target_features)}")

def ts_train_test_split(data, train_size, time_window=1, for_period=1):
    '''
    data: A  dataframe, with label to be predicted as first column.
    train_size is a floating value which specifies the size of the train dataset.
    For a 70:30 split, pass 0.7 as the argument for train_size,
    For a 80:20 split, pass 0.8 as the argument for train_size.
    time_window > for_period
    '''
    data_columns = data.columns
    train_data = data[:int(len(data) * train_size)]
    test_data = data[int(len(data) * train_size):]
    
    X_train = []
    Y_train = []
    X_test = []
    Y_test = []
    
    train_len = len(train_data)
    for row in range(time_window, train_len-for_period):
        X_train.append(np.array(train_data.iloc[row-time_window:row,:]))
    for row in range(time_window, train_len-for_period):
        Y_train.append(np.array(train_data.iloc[row:row+for_period,0]))
    
    test_len = len(test_data)
    for row in range(time_window, test_len-for_period):
        X_test.append(np.array(test_data.iloc[row-time_window:row,:]))
    for row in range(time_window, test_len-for_period):
        Y_test.append(np.array(test_data.iloc[row:row+for_period,0]))
    
    return np.array(X_train), np.array(X_test), np.array(Y_train), np.array(Y_test)

"""look_back = 10
X_train_gru_source, X_test_gru_source, Y_train_gru_source, Y_test_gru_source = ts_train_test_split(source_data, 0.7, look_back, 1)

### Training GRU model on source data
"""

X_train_gru_source, X_test_gru_source, Y_train_gru_source, Y_test_gru_source = train_test_split(source_features, source_labels, test_size=0.30, random_state=42, shuffle=False)

X_train_gru_source = np.array(X_train_gru_source)
X_test_gru_source = np.array(X_test_gru_source)
Y_train_gru_source = np.array(Y_train_gru_source)
Y_test_gru_source = np.array(Y_test_gru_source)

X_train_gru_source = X_train_gru_source.reshape(X_train_gru_source.shape[0], X_train_gru_source.shape[1], 1)
X_test_gru_source = X_test_gru_source.reshape(X_test_gru_source.shape[0], X_test_gru_source.shape[1], 1)
# Y_train_gru_source =  Y_train_gru_source.reshape(-1, time_window_size, 1)
# Y_test_gru_source =  Y_test_gru_source.reshape(-1, time_window_size, 1)

"""look_back = X_train_gru_source.shape[1]
feature_count = X_train_gru_source.shape[2]

X_train_gru_source = X_train_gru_source.reshape(-1, look_back, feature_count)
X_test_gru_source = X_test_gru_source.reshape(-1, look_back, feature_count)
# Y_train_gru_source =  Y_train_gru_source.reshape(-1, time_window_size, 1)
# Y_test_gru_source =  Y_test_gru_source.reshape(-1, time_window_size, 1)

#### Obtaining number of features
"""

feature_count = len(source_features.columns)

"""# See above 'Obtaining number of features'

#### Setting batch size

batch_size=3

#### Finding best hyperparams for source model

*Callback function used by tuner for early stopping*
"""

# patience is the number of epochs tolerated without an improvement in the loss.
stop_early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)

"""
Function used by Hyperband for hyperparameter tuning.
Returns a model.
"""
def source_model_builder(hp):                                           
    model = Sequential()
    """
    A Sequential gru_model_source is appropriate for a plain stack
    of layers where each layer has exactly one input tensor and
    one output tensor.
    """ 
    hp_unit = hp.Int('units', min_value=12, max_value=64, step=2)
    
    model.add(GRU(units=hp_unit, return_sequences=True,input_shape=(feature_count, 1)))
    model.add(GRU(units=hp_unit,return_sequences=True))
    model.add(GRU(units=hp_unit))
    model.add(Dense(1, activation="relu"))
    
    model.compile(optimizer=tf.keras.optimizers.Adam(), loss="mse", metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])
    
    return model

"""'''
Function used by Hyperband for hyperparameter tuning.
Returns a model.
'''
def source_model_builder(hp):                                           
    model = Sequential()
    '''
    A Sequential gru_model_source is appropriate for a plain stack
    of layers where each layer has exactly one input tensor and
    one output tensor.
    ''' 
    hp_unit = hp.Int('units', min_value=16, max_value=128, step=16)
    
    model.add(GRU(units=hp_unit, return_sequences=True,batch_input_shape=(batch_size, look_back, feature_count), stateful=True))
    model.add(GRU(units=hp_unit,return_sequences=True, stateful=True))
    model.add(GRU(units=hp_unit, stateful=True))
    model.add(Dense(1, kernel_initializer="uniform", activation="relu"))
    
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3, 1e-4])

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), loss="mse", metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])
    
    return model
"""

# pass the argument overWrite=True to avoid reusing results.
source_tuner = kt.Hyperband(source_model_builder,
                     objective='loss',
                     max_epochs=250,
                     factor=2,
                     seed=42,
                     hyperband_iterations=3,
                     directory='hyperparams',
                     project_name='source_stock_1')

"""# pass the argument overwrite=True to avoid reusing results.
source_tuner = kt.Hyperband(source_model_builder,
                     objective='loss',
                     max_epochs=80,
                     factor=2,
                     seed=42,
                     hyperband_iterations=3,
                     directory='hyperparams',
                     project_name='source_stock_1',
                     overwrite=True)

See: [https://keras.io/api/keras_tuner/tuners/hyperband/](https://keras.io/api/keras_tuner/tuners/hyperband/)
"""

source_tuner.search(X_train_gru_source, Y_train_gru_source, epochs=250)

# Get the optimal hyperparameters
source_best_hps=source_tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the GRU layer
is {source_best_hps.get('units')}.
""")

"""X_train_gru_source.shape

Y_train_gru_source.shape

source_tuner.search(X_train_gru_source, Y_train_gru_source, epochs=100, batch_size=batch_size)

# Get the optimal hyperparameters
source_best_hps=source_tuner.get_best_hyperparameters(num_trials=1)[0]

print(f'''
The hyperparameter search is complete. The optimal number of units in the GRU layer
is {source_best_hps.get('units')}.
''')

gru_model_source = source_tuner.hypermodel.build(source_best_hps)
gru_model_source.fit(X_train_gru_source, Y_train_gru_source, epochs=200, verbose=1, batch_size=3)
"""

gru_model_source = Sequential()
#A Sequential gru_model_source is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.
gru_model_source.add(GRU(80, return_sequences=True,input_shape=(feature_count,1)))
gru_model_source.add(GRU(80))
gru_model_source.add(Dense(1, activation="relu"))
gru_model_source.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_squared_error', metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])

gru_model_source.fit(X_train_gru_source,Y_train_gru_source,epochs=200,verbose=1)

Y_pred_test_gru_source = gru_model_source.predict(X_test_gru_source)
Y_pred_train_gru_source = gru_model_source.predict(X_train_gru_source)

"""Y_pred_test_gru_source = gru_model_source.predict(X_test_gru_source, batch_size=3)
Y_pred_train_gru_source = gru_model_source.predict(X_train_gru_source, batch_size=3)
"""

# Calculating RMSE
test_rmse_gru_source = sqrt(mean_squared_error(Y_pred_test_gru_source, Y_test_gru_source))
train_rmse_gru_source = sqrt(mean_squared_error(Y_pred_train_gru_source, Y_train_gru_source))
print("test_rmse_gru_source = {}, train_rmse_gru_source = {}".format(test_rmse_gru_source, train_rmse_gru_source))

# Calculating r2_score
test_r2_score_gru_source = r2_score(Y_test_gru_source, Y_pred_test_gru_source)
train_r2_score_gru_source = r2_score(Y_train_gru_source, Y_pred_train_gru_source)
print("test_r2_score_gru_source = {}, train_r2_score_gru_source = {}".format(test_r2_score_gru_source, train_r2_score_gru_source))

# Calculating MAE
test_mae_gru_source = mean_absolute_error(Y_test_gru_source, Y_pred_test_gru_source)
train_mae_gru_source = mean_absolute_error(Y_train_gru_source, Y_pred_train_gru_source)
print("test_mae_gru_source = {}, train_mae_gru_source = {}".format(test_mae_gru_source, train_mae_gru_source))

# Calculating MAPE
test_loss_gru_source, test_mae_gru_source, test_mape_gru_source = gru_model_source.evaluate(X_test_gru_source, Y_test_gru_source, verbose=0)
train_loss_gru_source, train_mae_gru_source, train_mape_gru_source = gru_model_source.evaluate(X_train_gru_source, Y_train_gru_source, verbose=0)
print("test_mape_gru_source = {}, train_mape_gru_source = {}".format(test_mape_gru_source, train_mape_gru_source))

"""# Calculating RMSE
test_rmse_gru_source = sqrt(mean_squared_error(Y_pred_test_gru_source, Y_test_gru_source))
train_rmse_gru_source = sqrt(mean_squared_error(Y_pred_train_gru_source, Y_train_gru_source))
print("test_rmse_gru_source = {}, train_rmse_gru_source = {}".format(test_rmse_gru_source, train_rmse_gru_source))

# Calculating r2_score
test_r2_score_gru_source = r2_score(Y_test_gru_source, Y_pred_test_gru_source)
train_r2_score_gru_source = r2_score(Y_train_gru_source, Y_pred_train_gru_source)
print("test_r2_score_gru_source = {}, train_r2_score_gru_source = {}".format(test_r2_score_gru_source, train_r2_score_gru_source))

# Calculating MAE
test_mae_gru_source = mean_absolute_error(Y_test_gru_source, Y_pred_test_gru_source)
train_mae_gru_source = mean_absolute_error(Y_train_gru_source, Y_pred_train_gru_source)
print("test_mae_gru_source = {}, train_mae_gru_source = {}".format(test_mae_gru_source, train_mae_gru_source))

# Calculating MAPE
test_loss_gru_source, test_mae_gru_source, test_mape_gru_source = gru_model_source.evaluate(X_test_gru_source, Y_test_gru_source, batch_size=3, verbose=0)
train_loss_gru_source, train_mae_gru_source, train_mape_gru_source = gru_model_source.evaluate(X_train_gru_source, Y_train_gru_source, batch_size=3, verbose=0)
print("test_mape_gru_source = {}, train_mape_gru_source = {}".format(test_mape_gru_source, train_mape_gru_source))
"""

plt.figure(1, figsize=(30, 6)) # Defines the figure size in inches
plt.plot(range(0, len(Y_test_gru_source)), Y_test_gru_source)
plt.plot(range(0, len(Y_test_gru_source)), Y_pred_test_gru_source)
plt.xlabel('Time')
plt.ylabel('Adj. Closing Price')
plt.title('Adj. Closing Price vs Time')
plt.legend(['Actual', 'Prediction'])

last_layer_source = gru_model_source.get_layer(index=2)
last_layer_source.get_weights()

"""#### Training source model on another stock having sufficient data"""

second_source_data = pd.read_csv('./final-data/indicators_source_tcs.csv', index_col=[0])

adj_close_second_source = second_source_data['Adj Close']

second_source_data.drop(['Date'], axis=1, inplace=True)

# temp
# Dropping volume
second_source_data.drop(['Volume'], axis=1, inplace=True)

second_source_values = second_source_data.values
second_scaler = MinMaxScaler(feature_range=(0, 1))
second_scaler = second_scaler.fit(second_source_values)
normalized_second_source_values = second_scaler.transform(second_source_values)

normalized_second_source = pd.DataFrame(normalized_second_source_values, columns=second_source_data.columns)

label_normalized_second_source = normalized_second_source['Adj Close']
normalized_second_source.drop(['Adj Close'], axis=1, inplace=True)

features_normalized_second_source = normalized_second_source

X_train_gru_second_source, X_test_gru_second_source, Y_train_gru_second_source, Y_test_gru_second_source = train_test_split(features_normalized_second_source, label_normalized_second_source, test_size=0.30, random_state=42, shuffle=False)

Y_train_gru_second_source.shape

X_train_gru_second_source = np.array(X_train_gru_second_source)
X_test_gru_second_source = np.array(X_test_gru_second_source)
Y_train_gru_second_source = np.array(Y_train_gru_second_source)
Y_test_gru_second_source = np.array(Y_test_gru_second_source)

X_train_gru_second_source = X_train_gru_second_source.reshape(X_train_gru_second_source.shape[0],X_train_gru_second_source.shape[1],1)
X_test_gru_second_source = X_test_gru_second_source.reshape(X_test_gru_second_source.shape[0],X_test_gru_second_source.shape[1],1)

history_gru_second_source = gru_model_source.fit(X_train_gru_second_source,Y_train_gru_second_source,validation_data=(X_test_gru_second_source,Y_test_gru_second_source),epochs=200,verbose=1)
Y_pred_test_gru_second_source = gru_model_source.predict(X_test_gru_second_source)
Y_pred_train_gru_second_source = gru_model_source.predict(X_train_gru_second_source)

# Calculating RMSE
test_rmse_gru_second_source = sqrt(mean_squared_error(Y_pred_test_gru_second_source, Y_test_gru_second_source))
train_rmse_gru_second_source = sqrt(mean_squared_error(Y_pred_train_gru_second_source, Y_train_gru_second_source))
print("test_rmse_gru_second_source = {}, train_rmse_gru_second_source = {}".format(test_rmse_gru_second_source, train_rmse_gru_second_source))

# Calculating r2_score
test_r2_score_gru_second_source = r2_score(Y_test_gru_second_source, Y_pred_test_gru_second_source)
train_r2_score_gru_second_source = r2_score(Y_train_gru_second_source, Y_pred_train_gru_second_source)
print("test_r2_score_gru_second_source = {}, train_r2_score_gru_second_source = {}".format(test_r2_score_gru_second_source, train_r2_score_gru_second_source))

# Calculating MAE
test_mae_gru_second_source = mean_absolute_error(Y_test_gru_second_source, Y_pred_test_gru_second_source)
train_mae_gru_second_source = mean_absolute_error(Y_train_gru_second_source, Y_pred_train_gru_second_source)
print("test_mae_gru_second_source = {}, train_mae_gru_second_source = {}".format(test_mae_gru_second_source, train_mae_gru_second_source))

# Calculating MAPE
test_loss_gru_second_source, test_mae_gru_second_source, test_mape_gru_second_source = gru_model_source.evaluate(X_test_gru_second_source, Y_test_gru_second_source, verbose=0)
train_loss_gru_second_source, train_mae_gru_second_source, train_mape_gru_second_source = gru_model_source.evaluate(X_train_gru_second_source, Y_train_gru_second_source, verbose=0)
print("test_mape_gru_second_source = {}, train_mape_gru_second_source = {}".format(test_mape_gru_second_source, train_mape_gru_second_source))

plt.figure(1, figsize=(30, 6)) # Defines the figure size in inches
plt.plot(range(0, len(Y_test_gru_second_source)), Y_test_gru_second_source)
plt.plot(range(0, len(Y_test_gru_second_source)), Y_pred_test_gru_second_source)
plt.xlabel('Time')
plt.ylabel('Adj. Closing Price')
plt.title('Adj. Closing Price vs Time')
plt.legend(['Actual', 'Prediction'])

"""### Transfer Learning"""

print("weights:", len(gru_model_source.weights))
print("trainable_weights:", len(gru_model_source.trainable_weights))
print("non_trainable_weights:", len(gru_model_source.non_trainable_weights))

'''
Freezing weights. Instance-based transfer learning.
'''
gru_model_source.trainable = False

"""#### Train-test split for target stock data"""

X_train_gru_target, X_test_gru_target, Y_train_gru_target, Y_test_gru_target = train_test_split(target_features, target_labels, test_size=0.30, random_state=42, shuffle=False)

X_train_gru_target = np.array(X_train_gru_target)
X_test_gru_target = np.array(X_test_gru_target)
Y_train_gru_target = np.array(Y_train_gru_target)
Y_test_gru_target = np.array(Y_test_gru_target)

X_train_gru_target = X_train_gru_target.reshape(X_train_gru_target.shape[0],X_train_gru_target.shape[1],1)
X_test_gru_target = X_test_gru_target.reshape(X_test_gru_target.shape[0],X_test_gru_target.shape[1],1)

Y_test_gru_target.shape

X_train_gru_target.shape

def target_model_builder(hp):                                           
    model = Sequential()
    model.add(gru_model_source.layers[0])
    
    hp_unit = hp.Int('units', min_value=12, max_value=64, step=2)
    model.add(GRU(units=hp_unit, return_sequences=True))
    model.add(GRU(units=hp_unit))
    model.add(Dense(1, activation="relu"))
    
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), loss="mse", metrics=['mean_absolute_percentage_error'])
    return model

target_tuner = kt.Hyperband(target_model_builder,
                     objective='loss',
                     max_epochs=200,
                     factor=2,
                     seed=42,
                     hyperband_iterations=3,
                     directory='hyperparams',
                     project_name='target_stock')

target_tuner.search(X_train_gru_target, Y_train_gru_target, epochs=250)

# Get the optimal hyperparameters
best_hps_target=target_tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps_target.get('units')} and the
optimal learning rate for the optimizer
is {best_hps_target.get('learning_rate')}.
""")

gru_model_target = Sequential()
gru_model_target.add(gru_model_source.layers[0])
gru_model_target.add(GRU(units=22, return_sequences=True, name="second"))
gru_model_target.add(GRU(units=22, name="third"))
gru_model_target.add(Dense(1, activation="relu", name="fourth"))
gru_model_target.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mean_squared_error', metrics=['mean_absolute_percentage_error'])

gru_model_target.fit(X_train_gru_target,Y_train_gru_target,epochs=200,verbose=1)

Y_pred_test_gru_target = gru_model_target.predict(X_test_gru_target)
Y_pred_train_gru_target = gru_model_target.predict(X_train_gru_target)

Y_test_gru_target.shape

Y_pred_test_gru_target.shape

# Calculating RMSE
test_rmse_gru_target = sqrt(mean_squared_error(Y_pred_test_gru_target, Y_test_gru_target))
train_rmse_gru_target = sqrt(mean_squared_error(Y_pred_train_gru_target, Y_train_gru_target))
print("test_rmse_gru_target = {}, train_rmse_gru_target = {}".format(test_rmse_gru_target, train_rmse_gru_target))

# Calculating r2_score
test_r2_score_gru_target = r2_score(Y_test_gru_target, Y_pred_test_gru_target)
train_r2_score_gru_target = r2_score(Y_train_gru_target, Y_pred_train_gru_target)
print("test_r2_score_gru_target = {}, train_r2_score_gru_target = {}".format(test_r2_score_gru_target , train_r2_score_gru_target))

# Calculating MAE
test_mae_gru_target = mean_absolute_error(Y_test_gru_target,Y_pred_test_gru_target)
train_mae_gru_target = mean_absolute_error(Y_train_gru_target,Y_pred_train_gru_target)
print("test_mae_gru_target = {}, train_mae_gru_target = {}".format(test_mae_gru_target , train_mae_gru_target))

# Calculating MAPE
test_loss_gru_target, test_mape_gru_target = gru_model_target.evaluate(X_test_gru_target, Y_test_gru_target, verbose=0)
train_loss_gru_target, train_mape_gru_target = gru_model_target.evaluate(X_train_gru_target, Y_train_gru_target, verbose=0)
print("test_mape_gru_target = {}, train_mape_gru_target = {}".format(test_mape_gru_target, train_mape_gru_target))

plt.figure(1, figsize=(30, 6)) # Defines the figure size in inches
plt.plot(range(0, len(Y_test_gru_target)), Y_test_gru_target)
plt.plot(range(0, len(Y_test_gru_target)), Y_pred_test_gru_target)
plt.xlabel('Time')
plt.ylabel('Adj. Closing Price')
plt.title('Adj. Closing Price vs Time')
plt.legend(['Actual', 'Prediction'])

"""#### Unfreezing weights of source model"""

gru_model_source.trainable = True

"""#### Setting a small learning rate for the target model

opt = tf.keras.optimizers.Adam(learning_rate=0.000009)

gru_model_target_transfer = Sequential()
gru_model_target_transfer.add(gru_model_source.layers[0])
gru_model_target_transfer.add(GRU(units=28, return_sequences=True, name="second"))
gru_model_target_transfer.add(GRU(units=28, name="third"))
gru_model_target_transfer.add(Dense(1, activation="relu", name="fourth"))
# gru_model_target.compile(optimizer="adam", loss='mean_squared_error', metrics=['mean_absolute_percentage_error'])
gru_model_target_transfer.compile(optimizer=opt, loss='mean_squared_error', metrics=['mean_absolute_percentage_error'])
"""

X_train_gru_target, X_test_gru_target, Y_train_gru_target, Y_test_gru_target = train_test_split(target_features, target_labels, test_size=0.30, random_state=42, shuffle=False)

X_train_gru_target = np.array(X_train_gru_target)
X_test_gru_target = np.array(X_test_gru_target)
Y_train_gru_target = np.array(Y_train_gru_target)
Y_test_gru_target = np.array(Y_test_gru_target)

X_train_gru_target = X_train_gru_target.reshape(X_train_gru_target.shape[0],X_train_gru_target.shape[1],1)
X_test_gru_target = X_test_gru_target.reshape(X_test_gru_target.shape[0],X_test_gru_target.shape[1],1)

"""#### Hyper parameter tuning for learning rate of target model"""

"""
Function used by Hyperband for hyperparameter tuning.
Returns a model.
"""
def target_model_builder_learning_rate(hp):                                           
    model = Sequential()
    """
    A Sequential gru_model_source is appropriate for a plain stack
    of layers where each layer has exactly one input tensor and
    one output tensor.
    """ 
    model.add(GRU(units=28, return_sequences=True,input_shape=(feature_count, 1)))
    model.add(GRU(units=28,return_sequences=True))
    model.add(GRU(units=28))
    model.add(Dense(1, activation="relu"))
    
    hp_learning_rate = hp.Choice('learning_rate', values=[0.001, 0.0005, 0.0001, 0.0005, 0.0001])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), loss="mse", metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])
    
    return model

target_tuner_for_learning_rate = kt.Hyperband(target_model_builder_learning_rate,
                     objective='loss',
                     max_epochs=200,
                     factor=2,
                     seed=42,
                     hyperband_iterations=3,
                     directory='hyperparams',
                     project_name='target_stock_tuning')

target_tuner_for_learning_rate.search(X_train_gru_target, Y_train_gru_target, epochs=250)

# Get the optimal hyperparameters
best_hps_target=target_tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal learning rate for the optimizer
is {best_hps_target.get('learning_rate')}.
""")

target_tuner.search_space_summary()

gru_models_target_best = target_tuner.get_best_models(num_models=2)

gru_models_target_best[0].fit(X_train_gru_target,Y_train_gru_target,validation_data=(X_test_gru_target,Y_test_gru_target),epochs=200,verbose=1)

Y_pred_test_gru_target_transfer_best = gru_models_target_best[0].predict(X_test_gru_target)
Y_pred_train_gru_target_transfer_best = gru_models_target_best[0].predict(X_train_gru_target)

# Calculating RMSE
test_rmse_gru_target_transfer_best = sqrt(mean_squared_error(Y_pred_test_gru_target_transfer_best, Y_test_gru_target))
train_rmse_gru_target_transfer_best = sqrt(mean_squared_error(Y_pred_train_gru_target_transfer_best, Y_train_gru_target))
print("test_rmse_gru_target_transfer_best = {}, train_rmse_gru_target_transfer_best = {}".format(test_rmse_gru_target_transfer_best, train_rmse_gru_target_transfer_best))

# Calculating r2_score
test_r2_score_gru_target = r2_score(Y_test_gru_target, Y_pred_test_gru_target)
train_r2_score_gru_target = r2_score(Y_train_gru_target, Y_pred_train_gru_target)
print("test_r2_score_gru_target = {}, train_r2_score_gru_target = {}".format(test_r2_score_gru_target , train_r2_score_gru_target))

# Calculating MAE
test_mae_target_transfer_best = sqrt(mean_squared_error(Y_pred_test_gru_target_transfer_best, Y_test_gru_target))
train_mae_target_transfer_best = sqrt(mean_squared_error(Y_pred_train_gru_target_transfer_best, Y_train_gru_target))
print("test_mae_target_transfer_best = {}, train_mae_target_transfer_best = {}".format(test_mae_target_transfer_best, train_mae_target_transfer_best))

# Calculating MAPE
test_loss_target_transfer_best, test_mape_target_transfer_best = gru_model_target.evaluate(X_test_gru_target, Y_test_gru_target, verbose=0)
train_loss_target_transfer_best, train_mape_target_transfer_best = gru_model_target.evaluate(X_train_gru_target, Y_train_gru_target, verbose=0)
print("test_mape_target_transfer_best = {}, train_mape_target_transfer_best = {}".format(test_mape_target_transfer_best, train_mape_target_transfer_best))

plt.figure(1, figsize=(30, 6)) # Defines the figure size in inches
plt.plot(range(0, len(Y_test_gru_target)), Y_test_gru_target)
plt.plot(range(0, len(Y_test_gru_target)), Y_pred_test_gru_target_transfer_best)
plt.xlabel('Time')
plt.ylabel('Adj. Closing Price')
plt.title('Adj. Closing Price vs Time')
plt.legend(['Actual', 'Prediction'])

"""#### Explainable AI (post-hoc model explanation using LIME)

#### Imports
"""

import lime
import lime.lime_tabular